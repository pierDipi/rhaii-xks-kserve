apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: kserve-controller-manager
    app.kubernetes.io/managed-by: kserve-controller-manager
    app.kubernetes.io/name: kserve-controller-manager
    app.kubernetes.io/part-of: kserve
  name: kserve-controller-manager
  namespace: opendatahub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-leader-election-role
  namespace: opendatahub
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - list
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - configmaps/status
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
---
aggregationRule:
  clusterRoleSelectors:
    - matchLabels:
        rbac.authorization.k8s.io/aggregate-to-kserve-admin: "true"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
  name: kserve-admin
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-kserve-admin: "true"
  name: kserve-edit
rules:
  - apiGroups:
      - serving.kserve.io
    resources:
      - inferenceservices
      - llminferenceservices
      - servingruntimes
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-manager-role
rules:
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - events
      - secrets
      - serviceaccounts
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - mutatingwebhookconfigurations
      - validatingwebhookconfigurations
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - authentication.k8s.io
    resources:
      - subjectaccessreviews
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
    resources:
      - gatewayclasses
      - gateways
      - httproutes
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - inference.networking.k8s.io
    resources:
      - inferencepools
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - inference.networking.x-k8s.io
    resources:
      - inferencemodels
      - inferenceobjectives
      - inferencepools
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - keda.sh
    resources:
      - scaledobjects
      - scaledobjects/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - keda.sh
    resources:
      - scaledobjects/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - leaderworkerset.x-k8s.io
    resources:
      - leaderworkersets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
      - servicemonitors
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - networking.istio.io
    resources:
      - destinationrules
      - virtualservices
      - virtualservices/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - networking.istio.io
    resources:
      - virtualservices/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors
      - opentelemetrycollectors/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - rbac.authorization.k8s.io
    resourceNames:
      - kserve-inferencegraph-auth-verifiers
    resources:
      - clusterrolebindings
    verbs:
      - create
      - get
      - patch
      - update
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - rolebindings
      - roles
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - route.openshift.io
    resources:
      - routes/status
    verbs:
      - get
  - apiGroups:
      - security.openshift.io
    resourceNames:
      - openshift-ai-llminferenceservice-scc
    resources:
      - securitycontextconstraints
    verbs:
      - use
  - apiGroups:
      - serving.knative.dev
    resources:
      - services
      - services/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - serving.knative.dev
    resources:
      - services/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - serving.kserve.io
    resources:
      - clusterservingruntimes
      - clusterservingruntimes/finalizers
      - clusterstoragecontainers
      - inferencegraphs
      - inferencegraphs/finalizers
      - inferenceservices
      - inferenceservices/finalizers
      - llminferenceserviceconfigs
      - llminferenceservices
      - servingruntimes
      - servingruntimes/finalizers
      - trainedmodels
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - serving.kserve.io
    resources:
      - clusterservingruntimes/status
      - inferencegraphs/status
      - inferenceservices/status
      - llminferenceservices/status
      - servingruntimes/status
      - trainedmodels/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - serving.kserve.io
    resources:
      - llminferenceserviceconfigs/finalizers
      - llminferenceservices/finalizers
    verbs:
      - update
  - apiGroups:
      - serving.kserve.io
    resources:
      - localmodelcaches
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: llm-monitoring
    app.kubernetes.io/part-of: kserve
  name: kserve-metrics-reader-cluster-role
rules:
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-proxy-role
rules:
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
  name: kserve-view
rules:
  - apiGroups:
      - serving.kserve.io
    resources:
      - servingruntimes
      - servingruntimes/status
      - servingruntimes/finalizers
      - inferenceservices
      - inferenceservices/status
      - inferenceservices/finalizers
      - llminferenceservices
      - llminferenceservices/status
      - llminferenceservices/finalizers
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-leader-election-rolebinding
  namespace: opendatahub
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kserve-leader-election-role
subjects:
  - kind: ServiceAccount
    name: kserve-controller-manager
    namespace: opendatahub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kserve-manager-role
subjects:
  - kind: ServiceAccount
    name: kserve-controller-manager
    namespace: opendatahub
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-proxy-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kserve-proxy-role
subjects:
  - kind: ServiceAccount
    name: kserve-controller-manager
    namespace: opendatahub
---
apiVersion: v1
data:
  _example: "################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# ====================================== EXPLAINERS CONFIGURATION ======================================\n# Example\nexplainers: |-\n  {\n      \"art\": {\n          \"image\" : \"kserve/art-explainer\",\n          \"defaultImageVersion\": \"latest\"\n      }\n  }\n# Art Explainer runtime configuration\n explainers: |-\n   {\n       # Art explainer runtime configuration\n       \"art\": {\n           # image contains the default Art explainer serving runtime image uri.\n           \"image\" : \"kserve/art-explainer\",\n   \n           # defautltImageVersion contains the Art explainer serving runtime default image version.\n           \"defaultImageVersion\": \"latest\"\n       }\n   }\n# ====================================== ISVC CONFIGURATION ======================================\n# Example - setting custom annotation  \n inferenceService: |-\n   {\n     \"serviceAnnotationDisallowedList\": [\n        \"my.custom.annotation/1\"  \n     ],\n     \"serviceLabelDisallowedList\": [\n        \"my.custom.label.1\"  \n     ]\n   }\n# Example - setting custom annotation\ninferenceService: |-\n  {\n    # ServiceAnnotationDisallowedList is a list of annotations that are not allowed to be propagated to Knative \n    # revisions, which prevents the reconciliation loop to be triggered if the annotations is \n    # configured here are used.\n    # Default values are:\n    #  \"autoscaling.knative.dev/min-scale\",\n    #  \"autoscaling.knative.dev/max-scale\",\n    #  \"internal.serving.kserve.io/storage-initializer-sourceuri\",\n    #  \"kubectl.kubernetes.io/last-applied-configuration\"\n    # Any new value will be appended to the list.\n    \"serviceAnnotationDisallowedList\": [\n      \"my.custom.annotation/1\"  \n    ],\n    # ServiceLabelDisallowedList is a list of labels that are not allowed to be propagated to Knative revisions\n    # which prevents the reconciliation loop to be triggered if the labels is configured here are used.\n    \"serviceLabelDisallowedList\": [\n      \"my.custom.label.1\"  \n    ]\n  } \n# Example - setting custom resource\ninferenceService: |-\n  {\n    \"resource\": {\n      \"cpuLimit\": \"1\",\n      \"memoryLimit\": \"2Gi\",\n      \"cpuRequest\": \"1\",\n      \"memoryRequest\": \"2Gi\"\n    }\n  }\n# Example - setting custom resource\ninferenceService: |-\n  {\n    # resource contains the default resource configuration for the inference service.\n    # you can override this configuration by specifying the resources in the inference service yaml.\n    # If you want to unbound the resource (limits and requests), you can set the value to null or \"\" \n    # or just remove the specific field from the config.\n    \"resource\": {\n       # cpuLimit is the limits.cpu to set for the inference service.\n       \"cpuLimit\": \"1\",\n\n       # memoryLimit is the limits.memory to set for the inference service.\n       \"memoryLimit\": \"2Gi\",\n\n       # cpuRequest is the requests.cpu to set for the inference service.\n       \"cpuRequest\": \"1\",\n\n       # memoryRequest is the requests.memory to set for the inference service.\n       \"memoryRequest\": \"2Gi\"\n    }\n }\n# ====================================== MultiNode CONFIGURATION ======================================\n# Example   \nmultiNode: |-\n  {\n    \"customGPUResourceTypeList\": [\n      \"custom.com/gpu\"\n    ]\n  }\n# Example of multinode configuration\nmultiNode: |-\n  {      \n    # CustomGPUResourceTypeList is a list of custom GPU resource types intended to identify the GPU type of a resource,\n    # not to restrict the user from using a specific GPU type.\n    # The MultiNode runtime pod will dynamically add GPU resources based on the registered GPU types.\n    \"customGPUResourceTypeList\": [\n      \"custom.com/gpu\"\n    ]\n  }  \n # ====================================== OTelCollector CONFIGURATION ======================================\n # Example\n opentelemetryCollector: |-\n   {\n     # scrapeInterval is the interval at which the OpenTelemetry Collector will scrape the metrics.\n     \"scrapeInterval\": \"5s\",\n     # metricScalerEndpoint is the endpoint from which the KEDA's ScaledObject will scrape the metrics.\n     \"metricScalerEndpoint\": \"keda-otel-scaler.keda.svc:4318\",\n     # metricReceiverEndpoint is the endpoint from which the OpenTelemetry Collector will scrape the metrics.\n      \"metricReceiverEndpoint\": \"keda-otel-scaler.keda.svc:4317\"\n   }\n  \n # ====================================== STORAGE INITIALIZER CONFIGURATION ======================================\n # Example\n storageInitializer: |-\n   {\n       \"image\" : \"kserve/storage-initializer:latest\",\n       \"memoryRequest\": \"100Mi\",\n       \"memoryLimit\": \"1Gi\",\n       \"cpuRequest\": \"100m\",\n       \"cpuLimit\": \"1\",\n       \"caBundleConfigMapName\": \"\",\n       \"caBundleVolumeMountPath\": \"/etc/ssl/custom-certs\",\n       \"enableDirectPvcVolumeMount\": false,\n       \"enableModelcar\": false,\n       \"cpuModelcar\": \"10m\",\n       \"memoryModelcar\": \"15Mi\"\n   }\n storageInitializer: |-\n   {\n       # image contains the default storage initializer image uri.\n       \"image\" : \"kserve/storage-initializer:latest\",\n       \n       # memoryRequest is the requests.memory to set for the storage initializer init container.\n       \"memoryRequest\": \"100Mi\",\n   \n        # memoryLimit is the limits.memory to set for the storage initializer init container.\n       \"memoryLimit\": \"1Gi\",\n       \n       # cpuRequest is the requests.cpu to set for the storage initializer init container.\n       \"cpuRequest\": \"100m\",\n       \n       # cpuLimit is the limits.cpu to set for the storage initializer init container.\n       \"cpuLimit\": \"1\",\n   \n       # caBundleConfigMapName is the ConfigMap will be copied to a user namespace for the storage initializer init container.\n       \"caBundleConfigMapName\": \"\",\n\n       # caBundleVolumeMountPath is the mount point for the configmap set by caBundleConfigMapName for the storage initializer init container.\n       \"caBundleVolumeMountPath\": \"/etc/ssl/custom-certs\",\n\n       # enableDirectPvcVolumeMount controls whether users can mount pvc volumes directly.\n       # if pvc volume is provided in storageuri then the pvc volume is directly mounted to /mnt/models in the user container.\n       # rather than symlink it to a shared volume. For more info see https://github.com/kserve/kserve/issues/2737\n       \"enableDirectPvcVolumeMount\": true,\n\n       # enableModelcar enabled allows you to directly access an OCI container image by\n       # using a source URL with an \"oci://\" schema.\n       \"enableModelcar\": false,\n\n       # cpuModelcar is the cpu request and limit that is used for the passive modelcar container. It can be\n       # set very low, but should be allowed by any Kubernetes LimitRange that might apply.\n       \"cpuModelcar\": \"10m\",\n\n       # cpuModelcar is the memory request and limit that is used for the passive modelcar container. It can be\n       # set very low, but should be allowed by any Kubernetes LimitRange that might apply.\n       \"memoryModelcar\": \"15Mi\",\n\n       # uidModelcar is the UID under with which the modelcar process and the main container is running.\n       # Some Kubernetes clusters might require this to be root (0). If not set the user id is left untouched (default)\n       \"uidModelcar\": 10\n   }\n \n # ====================================== CREDENTIALS ======================================\n # Example\n credentials: |-\n   {\n      \"storageSpecSecretName\": \"storage-config\",\n      \"storageSecretNameAnnotation\": \"serving.kserve.io/storageSecretName\",\n      \"gcs\": {\n          \"gcsCredentialFileName\": \"gcloud-application-credentials.json\"\n      },\n      \"s3\": {\n          \"s3AccessKeyIDName\": \"AWS_ACCESS_KEY_ID\",\n          \"s3SecretAccessKeyName\": \"AWS_SECRET_ACCESS_KEY\",\n          \"s3Endpoint\": \"\",\n          \"s3UseHttps\": \"\",\n          \"s3Region\": \"\",\n          \"s3VerifySSL\": \"\",\n          \"s3UseVirtualBucket\": \"\",\n          \"s3UseAccelerate\": \"\",\n          \"s3UseAnonymousCredential\": \"\",\n          \"s3CABundleConfigMap\": \"\",\n          \"s3CABundle\": \"\"\n      }\n   }\n # This is a global configuration used for downloading models from the cloud storage.\n # You can override this configuration by specifying the annotations on service account or static secret.\n # https://kserve.github.io/website/master/modelserving/storage/s3/s3/\n # For a quick reference about AWS ENV variables:\n # AWS Cli: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html\n # Boto: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables\n #\n # The `s3AccessKeyIDName` and `s3SecretAccessKeyName` fields are only used from this configmap when static credentials (IAM User Access Key Secret)\n # are used as the authentication method for AWS S3.\n # The rest of the fields are used in both authentication methods (IAM Role for Service Account & IAM User Access Key Secret) if a non-empty value is provided.\n credentials: |-\n   {\n      # storageSpecSecretName contains the secret name which has the credentials for downloading the model.\n      # This option is used when specifying the storage spec on isvc yaml.\n      \"storageSpecSecretName\": \"storage-config\",\n\n      # The annotation can be specified on isvc yaml to allow overriding with the secret name reference from the annotation value.\n      # When using storageUri the order of the precedence is: secret name reference annotation > secret name references from service account\n      # When using storageSpec the order of the precedence is: secret name reference annotation > storageSpecSecretName in configmap\n\n      # Configuration for google cloud storage\n      \"gcs\": {\n          # gcsCredentialFileName specifies the filename of the gcs credential\n          \"gcsCredentialFileName\": \"gcloud-application-credentials.json\"\n      },\n      \n      # Configuration for aws s3 storage. This add the corresponding environmental variables to the storage initializer init container.\n      # For more info on s3 storage see https://kserve.github.io/website/master/modelserving/storage/s3/s3/\n      \"s3\": {\n          # s3AccessKeyIDName specifies the s3 access key id name\n          \"s3AccessKeyIDName\": \"AWS_ACCESS_KEY_ID\",\n   \n          # s3SecretAccessKeyName specifies the s3 secret access key name\n          \"s3SecretAccessKeyName\": \"AWS_SECRET_ACCESS_KEY\",\n          \n          # s3Endpoint specifies the s3 endpoint\n          \"s3Endpoint\": \"\",\n          \n          # s3UseHttps controls whether to use secure https or unsecure http to download models.\n          # Allowed values are 0 and 1.\n          \"s3UseHttps\": \"\",\n   \n          # s3Region specifies the region of the bucket.\n          \"s3Region\": \"\",\n          \n          # s3VerifySSL controls whether to verify the tls/ssl certificate.\n          \"s3VerifySSL\": \"\",\n          \n          # s3UseVirtualBucket configures whether it is a virtual bucket or not.\n          \"s3UseVirtualBucket\": \"\",\n\n          # s3UseAccelerate configures whether to use transfer acceleration.\n          \"s3UseAccelerate\": \"\",\n           \n          # s3UseAnonymousCredential configures whether to use anonymous credentials to download the model or not.\n          \"s3UseAnonymousCredential\": \"\",\n\n          # s3CABundleConfigMap specifies the mounted CA bundle config map name.\n          \"s3CABundleConfigMap\": \"\",\n          \n          # s3CABundle specifies the mounted path for the config map when used with a configured CA bundle config map.\n          # s3CABundle specifies the path to a certificate bundle to use for HTTPS certificate validation when used absent of a configured CA bundle config map.\n          \"s3CABundle\": \"\"\n      }\n   }\n \n # ====================================== INGRESS CONFIGURATION ======================================\n # Example\n ingress: |-\n   {    \n       \"enableGatewayApi\": false,\n       \"kserveIngressGateway\": \"kserve/kserve-ingress-gateway\",\n       \"ingressGateway\" : \"knative-serving/knative-ingress-gateway\",\n       \"localGateway\" : \"knative-serving/knative-local-gateway\",\n       \"localGatewayService\" : \"knative-local-gateway.istio-system.svc.cluster.local\",\n       \"ingressDomain\"  : \"example.com\",\n       \"additionalIngressDomains\": [\"additional-example.com\", \"additional-example-1.com\"],\n       \"ingressClassName\" : \"istio\",\n       \"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\n       \"urlScheme\": \"http\",\n       \"disableIstioVirtualHost\": false,\n       \"disableIngressCreation\": false\n   }\n ingress: |-\n   {   \n       # enableGatewayApi specifies whether to use Gateway API instead of Ingress to serve external traffic.\n       \"enableGatewayApi\": false,\n\n       # KServe implements [Gateway API](https://gateway-api.sigs.k8s.io/) to serve external traffic. \n       # By default, KServe configures a default gateway to serve external traffic.\n       # But, KServe can be configured to use a custom gateway by modifying this configuration.\n       # The gateway should be specified in format <gateway namespace>/<gateway name>\n       # NOTE: This configuration only applicable for raw deployment.\n       \"kserveIngressGateway\": \"kserve/kserve-ingress-gateway\",\n \n       # ingressGateway specifies the ingress gateway to serve external traffic.\n       # The gateway should be specified in format <gateway namespace>/<gateway name>\n       # NOTE: This configuration only applicable for serverless deployment with Istio configured as network layer.\n       \"ingressGateway\" : \"knative-serving/knative-ingress-gateway\",\n \n       # knativeLocalGatewayService specifies the hostname of the Knative's local gateway service.\n       # The default KServe configurations are re-using the Istio local gateways for Knative. In this case, this\n       # knativeLocalGatewayService field can be left unset. When unset, the value of \"localGatewayService\" will be used.\n       # However, sometimes it may be better to have local gateways specifically for KServe (e.g. when enabling strict mTLS in Istio).\n       # Under such setups where KServe is needed to have its own local gateways, the values of the \"localGateway\" and\n       # \"localGatewayService\" should point to the KServe local gateways. Then, this knativeLocalGatewayService field\n       # should point to the Knative's local gateway service.\n       # NOTE: This configuration only applicable for serverless deployment with Istio configured as network layer.\n       \"knativeLocalGatewayService\": \"\",\n \n       # localGateway specifies the gateway which handles the network traffic within the cluster.\n       # NOTE: This configuration only applicable for serverless deployment with Istio configured as network layer.\n       \"localGateway\" : \"knative-serving/knative-local-gateway\",\n \n       # localGatewayService specifies the hostname of the local gateway service.\n       # NOTE: This configuration only applicable for serverless deployment with Istio configured as network layer.\n       \"localGatewayService\" : \"knative-local-gateway.istio-system.svc.cluster.local\",\n \n       # ingressDomain specifies the domain name which is used for creating the url.\n       # If ingressDomain is empty then example.com is used as default domain.\n       # NOTE: This configuration only applicable for raw deployment.\n       \"ingressDomain\"  : \"example.com\",\n\n       # additionalIngressDomains specifies the additional domain names which are used for creating the url.\n       \"additionalIngressDomains\": [\"additional-example.com\", \"additional-example-1.com\"]\n\n       # ingressClassName specifies the ingress controller to use for ingress traffic.\n       # This is optional and if omitted the default ingress in the cluster is used.\n       # https://kubernetes.io/docs/concepts/services-networking/ingress/#default-ingress-class\n       # NOTE: This configuration only applicable for raw deployment.\n       \"ingressClassName\" : \"istio\",\n \n       # domainTemplate specifies the template for generating domain/url for each inference service by combining variable from:\n       # Name of the inference service  ( {{ .Name}} )\n       # Namespace of the inference service ( {{ .Namespace }} )\n       # Annotation of the inference service ( {{ .Annotations.key }} )\n       # Label of the inference service ( {{ .Labels.key }} )\n       # IngressDomain ( {{ .IngressDomain }} )\n       # If domain template is empty the default template {{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }} is used.\n       # NOTE: This configuration only applicable for raw deployment.\n       \"domainTemplate\": \"{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}\",\n \n       # urlScheme specifies the url scheme to use for inference service and inference graph.\n       # If urlScheme is empty then by default http is used.\n       \"urlScheme\": \"http\",\n \n       # disableIstioVirtualHost controls whether to use istio as network layer.\n       # By default istio is used as the network layer. When DisableIstioVirtualHost is true, KServe does not\n       # create the top level virtual service thus Istio is no longer required for serverless mode.\n       # By setting this field to true, user can use other networking layers supported by knative.\n       # For more info https://github.com/kserve/kserve/pull/2380, https://kserve.github.io/website/master/admin/serverless/kourier_networking/.\n       # NOTE: This configuration is only applicable to serverless deployment.\n       \"disableIstioVirtualHost\": false,\n\n       # disableIngressCreation controls whether to disable ingress creation for raw deployment mode.\n       \"disableIngressCreation\": false,\n \n       # pathTemplate specifies the template for generating path based url for each inference service.\n       # The following variables can be used in the template for generating url.\n       # Name of the inference service  ( {{ .Name}} )\n       # Namespace of the inference service ( {{ .Namespace }} )\n       # For more info https://github.com/kserve/kserve/issues/2257.\n       # NOTE: This configuration only applicable to serverless deployment.\n       \"pathTemplate\": \"/serving/{{ .Namespace }}/{{ .Name }}\"\n   }\n \n # ====================================== LOGGER CONFIGURATION ======================================\n # Example\n logger: |-\n   {\n       \"image\" : \"kserve/agent:latest\",\n       \"memoryRequest\": \"100Mi\",\n       \"memoryLimit\": \"1Gi\",\n       \"cpuRequest\": \"100m\",\n       \"cpuLimit\": \"1\",\n       \"defaultUrl\": \"http://default-broker\"\n   }\n logger: |-\n   {\n       # image contains the default logger image uri.\n       \"image\" : \"kserve/agent:latest\",\n   \n       # memoryRequest is the requests.memory to set for the logger container.\n       \"memoryRequest\": \"100Mi\",\n       \n       # memoryLimit is the limits.memory to set for the logger container.\n       \"memoryLimit\": \"1Gi\",\n       \n       # cpuRequest is the requests.cpu to set for the logger container.\n       \"cpuRequest\": \"100m\",\n       \n       # cpuLimit is the limits.cpu to set for the logger container.\n       \"cpuLimit\": \"1\",\n       \n       # defaultUrl specifies the default logger url. If logger is not specified in the resource this url is used.\n       \"defaultUrl\": \"http://default-broker\"\n   }\n \n # ====================================== BATCHER CONFIGURATION ======================================\n # Example\n batcher: |-\n   {\n       \"image\" : \"kserve/agent:latest\",\n       \"memoryRequest\": \"1Gi\",\n       \"memoryLimit\": \"1Gi\",\n       \"cpuRequest\": \"1\",\n       \"cpuLimit\": \"1\",\n       \"maxBatchSize\": \"32\",\n       \"maxLatency\": \"5000\"\n   }\n batcher: |-\n   {\n       # image contains the default batcher image uri.\n       \"image\" : \"kserve/agent:latest\",\n       \n       # memoryRequest is the requests.memory to set for the batcher container.\n       \"memoryRequest\": \"1Gi\",\n   \n       # memoryLimit is the limits.memory to set for the batcher container.\n       \"memoryLimit\": \"1Gi\",\n       \n       # cpuRequest is the requests.cpu to set for the batcher container.\n       \"cpuRequest\": \"1\",\n       \n       # cpuLimit is the limits.cpu to set for the batcher container.\n       \"cpuLimit\": \"1\"\n\n       # maxBatchSize is the default maximum batch size for batcher.\n       \"maxBatchSize\": \"32\",\n\n       # maxLatency is the default maximum latency in milliseconds for batcher to wait and collect the batch.\n       \"maxLatency\": \"5000\"\n   }\n \n # ====================================== AGENT CONFIGURATION ======================================\n # Example\n agent: |-\n   {\n       \"image\" : \"kserve/agent:latest\",\n       \"memoryRequest\": \"100Mi\",\n       \"memoryLimit\": \"1Gi\",\n       \"cpuRequest\": \"100m\",\n       \"cpuLimit\": \"1\"\n   }\n agent: |-\n   {\n       # image contains the default agent image uri.\n       \"image\" : \"kserve/agent:latest\",\n   \n       # memoryRequest is the requests.memory to set for the agent container.\n       \"memoryRequest\": \"100Mi\",\n   \n       # memoryLimit is the limits.memory to set for the agent container.\n       \"memoryLimit\": \"1Gi\",\n       \n       # cpuRequest is the requests.cpu to set for the agent container.\n       \"cpuRequest\": \"100m\",\n       \n       # cpuLimit is the limits.cpu to set for the agent container.\n       \"cpuLimit\": \"1\"\n   }\n \n # ====================================== ROUTER CONFIGURATION ======================================\n # Example\n router: |-\n   {\n       \"image\" : \"kserve/router:latest\",\n       \"memoryRequest\": \"100Mi\",\n       \"memoryLimit\": \"1Gi\",\n       \"cpuRequest\": \"100m\",\n       \"cpuLimit\": \"1\",\n       \"headers\": {\n         \"propagate\": []\n       },\n       \"imagePullPolicy\": \"IfNotPresent\",\n       \"imagePullSecrets\": [\"docker-secret\"]\n   }\n # router is the implementation of inference graph.\n router: |-\n   {\n       # image contains the default router image uri.\n       \"image\" : \"kserve/router:latest\",\n       \n       # memoryRequest is the requests.memory to set for the router container.\n       \"memoryRequest\": \"100Mi\",\n       \n       # memoryLimit is the limits.memory to set for the router container.\n       \"memoryLimit\": \"1Gi\",\n       \n       # cpuRequest is the requests.cpu to set for the router container.\n       \"cpuRequest\": \"100m\",\n       \n       # cpuLimit is the limits.cpu to set for the router container.\n       \"cpuLimit\": \"1\",\n       \n       # Propagate the specified headers to all the steps specified in an InferenceGraph. \n       # You can either specify the exact header names or use [Golang supported regex patterns]\n       # (https://pkg.go.dev/regexp/syntax@go1.21.3#hdr-Syntax) to propagate multiple headers.\n       \"headers\": {\n         \"propagate\": [\n            \"Authorization\",\n            \"Test-Header-*\",\n            \"*Trace-Id*\"\n         ]\n       }\n\n       # imagePullPolicy specifies when the router image should be pulled from registry.\n       \"imagePullPolicy\": \"IfNotPresent\",\n       \n       # # imagePullSecrets specifies the list of secrets to be used for pulling the router image from registry.\n       # https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n       \"imagePullSecrets\": [\"docker-secret\"]\n   }\n \n # ====================================== DEPLOYMENT CONFIGURATION ======================================\n # Example\n deploy: |-\n   {\n     \"defaultDeploymentMode\": \"Serverless\"\n   }\n deploy: |-\n   {\n     # defaultDeploymentMode specifies the default deployment mode of the kserve. The supported values are\n     # Serverless, RawDeployment and ModelMesh. Users can override the deployment mode at service level\n     # by adding the annotation serving.kserve.io/deploymentMode.For more info on deployment mode visit\n     # Serverless https://kserve.github.io/website/master/admin/serverless/serverless/\n     # RawDeployment https://kserve.github.io/website/master/admin/kubernetes_deployment/\n     # ModelMesh https://kserve.github.io/website/master/admin/modelmesh/\n     \"defaultDeploymentMode\": \"Serverless\"\n   }\n\n # ====================================== SERVICE CONFIGURATION ======================================\n # Example\n service: |-\n   {\n     \"serviceClusterIPNone\":  false\n   }\n service: |-\n   {\n      # ServiceClusterIPNone is a boolean flag to indicate if the service should have a clusterIP set to None.\n      # If the DeploymentMode is Raw, the default value for ServiceClusterIPNone if not set is false\n      # \"serviceClusterIPNone\":  false\n   }\n\n # ====================================== METRICS CONFIGURATION ======================================\n # Example\n metricsAggregator: |-\n   {\n     \"enableMetricAggregation\": \"false\",\n     \"enablePrometheusScraping\" : \"false\"\n   }\n # For more info see https://github.com/kserve/kserve/blob/master/qpext/README.md\n metricsAggregator: |-\n   {\n     # enableMetricAggregation configures metric aggregation annotation. This adds the annotation serving.kserve.io/enable-metric-aggregation to every\n     # service with the specified boolean value. If true enables metric aggregation in queue-proxy by setting env vars in the queue proxy container\n     # to configure scraping ports.\n     \"enableMetricAggregation\": \"false\",\n     \n     # enablePrometheusScraping configures metric aggregation annotation. This adds the annotation serving.kserve.io/enable-metric-aggregation to every\n     # service with the specified boolean value. If true, prometheus annotations are added to the pod. If serving.kserve.io/enable-metric-aggregation is false,\n     # the prometheus port is set with the default prometheus scraping port 9090, otherwise the prometheus port annotation is set with the metric aggregation port.\n     \"enablePrometheusScraping\" : \"false\"\n   }\n  \n # ====================================== LOCALMODEL CONFIGURATION ======================================\n # Example\n localModel: |-\n   {\n     \"enabled\": false,\n     # jobNamespace specifies the namespace where the download job will be created.\n     \"jobNamespace\": \"kserve-localmodel-jobs\",\n     # defaultJobImage specifies the default image used for the download job.\n     \"defaultJobImage\" : \"kserve/storage-initializer:latest\",\n     # Kubernetes modifies the filesystem group ID on the attached volume.\n     \"fsGroup\": 1000,\n     # TTL for the download job after it is finished.\n     \"jobTTLSecondsAfterFinished\": 3600,\n     # The frequency at which the local model agent reconciles the local models\n     # This is to detect if models are missing from local disk\n     \"reconcilationFrequencyInSecs\": 60,\n     # This is to disable localmodel pv and pvc management for namespaces without isvcs\n     \"disableVolumeManagement\": false\n   }"
  agent: |-
    {
        "image" : "registry.redhat.io/rhoai/odh-kserve-agent-rhel9@sha256:d0a98dce62227e4ff5877bfa53276a9e1cfd59b661eb085cff6153900e1c5ac1",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1"
    }
  batcher: |-
    {
        "image" : "registry.redhat.io/rhoai/odh-kserve-agent-rhel9@sha256:d0a98dce62227e4ff5877bfa53276a9e1cfd59b661eb085cff6153900e1c5ac1",
        "memoryRequest": "1Gi",
        "memoryLimit": "1Gi",
        "cpuRequest": "1",
        "cpuLimit": "1"
    }
  credentials: |-
    {
       "storageSpecSecretName": "storage-config",
       "storageSecretNameAnnotation": "serving.kserve.io/storageSecretName",
       "gcs": {
           "gcsCredentialFileName": "gcloud-application-credentials.json"
       },
       "s3": {
           "s3AccessKeyIDName": "AWS_ACCESS_KEY_ID",
           "s3SecretAccessKeyName": "AWS_SECRET_ACCESS_KEY",
           "s3Endpoint": "",
           "s3UseHttps": "",
           "s3Region": "",
           "s3VerifySSL": "",
           "s3UseVirtualBucket": "",
           "s3UseAccelerate": "",
           "s3UseAnonymousCredential": "",
           "s3CABundleConfigMap": "odh-kserve-custom-ca-bundle",
           "s3CABundle": "/etc/ssl/custom-certs/cabundle.crt"
       }
    }
  deploy: |-
    {
      "defaultDeploymentMode": "RawDeployment"
    }
  explainers: '{}'
  inferenceService: |-
    {
      "serviceAnnotationDisallowedList": [
        "autoscaling.knative.dev/min-scale",
        "autoscaling.knative.dev/max-scale",
        "internal.serving.kserve.io/storage-initializer-sourceuri",
        "kubectl.kubernetes.io/last-applied-configuration",
        "security.opendatahub.io/enable-auth",
        "networking.knative.dev/visibility",
        "opendatahub.io/hardware-profile-name",
        "opendatahub.io/hardware-profile-namespace"
      ]
    }
  ingress: |-
    {
        "enableGatewayApi": true,
        "kserveIngressGateway": "opendatahub/inference-gateway",
        "ingressGateway" : "knative-serving/knative-ingress-gateway",
        "knativeLocalGatewayService" : "knative-local-gateway.istio-system.svc.cluster.local",
        "ingressService" : "istio-ingressgateway.istio-system.svc.cluster.local",
        "localGateway" : "istio-system/kserve-local-gateway",
        "localGatewayService" : "kserve-local-gateway.istio-system.svc.cluster.local",
        "ingressDomain"  : "example.com",
        "ingressClassName" : "istio",
        "domainTemplate": "{{ .Name }}-{{ .Namespace }}.{{ .IngressDomain }}",
        "urlScheme": "https",
        "disableIstioVirtualHost": false,
        "disableIngressCreation": false
    }
  localModel: |-
    {
      "enabled": false,
      "jobNamespace": "kserve-localmodel-jobs",
      "defaultJobImage" : "kserve/storage-initializer:latest",
      "fsGroup": 1000
    }
  logger: |-
    {
        "image" : "registry.redhat.io/rhoai/odh-kserve-agent-rhel9@sha256:d0a98dce62227e4ff5877bfa53276a9e1cfd59b661eb085cff6153900e1c5ac1",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1",
        "defaultUrl": "http://default-broker"
    }
  metricsAggregator: |-
    {
      "enableMetricAggregation": "false",
      "enablePrometheusScraping" : "false"
    }
  oauthProxy: |-
    {
      "image" : "registry.redhat.io/rhoai/odh-kube-auth-proxy-rhel9@sha256:2c4be58b9cbbfbf0cce82771f9823f5df664a21c139feee9e4f8beb9cf3ad76a",
      "memoryRequest": "64Mi",
      "memoryLimit": "128Mi",
      "cpuRequest": "100m",
      "cpuLimit": "200m"
    }
  opentelemetryCollector: |-
    {
      "scrapeInterval": "5s",
      "metricReceiverEndpoint": "keda-otel-scaler.keda.svc:4317",
      "metricScalerEndpoint": "keda-otel-scaler.keda.svc:4318"
    }
  router: |-
    {
        "image" : "registry.redhat.io/rhoai/odh-kserve-router-rhel9@sha256:2966ea073e32fc27c19ddd3b4b1a36d4d1becc00e89be7b590029d61bcadcec6",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1",
        "headers": {
          "propagate": [
            "Authorization"
          ]
        }
    }
  security: |-
    {
      "autoMountServiceAccountToken": false
    }
  service: |-
    {
        "serviceClusterIPNone": true
    }
  storageInitializer: |-
    {
        "image" : "registry.redhat.io/rhoai/odh-kserve-storage-initializer-rhel9@sha256:21bd9dbd0ca651a62e299440c80f72ef3564ce22b0991c4f9797db5f988e1d64",
        "memoryRequest": "100Mi",
        "memoryLimit": "24Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1",
        "enableDirectPvcVolumeMount": true,
        "cpuModelcar": "10m",
        "memoryModelcar": "15Mi",
        "enableModelcar": true
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: inferenceservice-config
  namespace: opendatahub
---
apiVersion: v1
data:
  kserve-agent: registry.redhat.io/rhoai/odh-kserve-agent-rhel9@sha256:d0a98dce62227e4ff5877bfa53276a9e1cfd59b661eb085cff6153900e1c5ac1
  kserve-controller: registry.redhat.io/rhoai/odh-kserve-controller-rhel9@sha256:ef4d6c03cd06d18e52a0634d9bb9c2d2ed8223a11d9d583c4334cc92573849b5
  kserve-llm-d: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
  kserve-llm-d-amd-rocm: registry.redhat.io/rhaiis/vllm-rocm-rhel9@sha256:e345cf9453afae0d3c2afe2f7fb9be8fac772f46593b873e925d38ae2b3ee537
  kserve-llm-d-inference-scheduler: registry.redhat.io/rhoai/odh-llm-d-inference-scheduler-rhel9@sha256:905b704f7d9c68af1827bba7c6eefec22d269a9084791952221eb1a1d10e32c7
  kserve-llm-d-nvidia-cuda: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
  kserve-llm-d-routing-sidecar: registry.redhat.io/rhoai/odh-llm-d-routing-sidecar-rhel9@sha256:7f93742da18df2ce220cd8d6a0310c18af6fe04905c83f23d022e065716ebd88
  kserve-router: registry.redhat.io/rhoai/odh-kserve-router-rhel9@sha256:2966ea073e32fc27c19ddd3b4b1a36d4d1becc00e89be7b590029d61bcadcec6
  kserve-storage-initializer: registry.redhat.io/rhoai/odh-kserve-storage-initializer-rhel9@sha256:21bd9dbd0ca651a62e299440c80f72ef3564ce22b0991c4f9797db5f988e1d64
  kube-rbac-proxy: registry.redhat.io/rhoai/odh-kube-auth-proxy-rhel9@sha256:2c4be58b9cbbfbf0cce82771f9823f5df664a21c139feee9e4f8beb9cf3ad76a
kind: ConfigMap
metadata:
  name: kserve-parameters
  namespace: opendatahub
---
apiVersion: v1
data:
  CA_SECRET_NAME: opendatahub-ca
  CA_SECRET_NAMESPACE: cert-manager
  ISSUER_REF_GROUP: cert-manager.io
  ISSUER_REF_KIND: ClusterIssuer
  ISSUER_REF_NAME: opendatahub-ca-issuer
  ISTIO_CA_CERTIFICATE_PATH: /var/run/secrets/opendatahub/ca.crt
  NAMESPACE: opendatahub
kind: ConfigMap
metadata:
  name: odh-xks-kserve-parameters
  namespace: opendatahub
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-webhook-server-secret
  namespace: opendatahub
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "8443"
    prometheus.io/scheme: https
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
    controller-tools.k8s.io: "1.0"
  name: kserve-controller-manager-metrics-service
  namespace: opendatahub
spec:
  ports:
    - name: https
      port: 8443
      targetPort: https
  selector:
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
    controller-tools.k8s.io: "1.0"
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
    controller-tools.k8s.io: "1.0"
  name: kserve-controller-manager-service
  namespace: opendatahub
spec:
  ports:
    - port: 8443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
    controller-tools.k8s.io: "1.0"
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/part-of: kserve
  name: kserve-webhook-server-service
  namespace: opendatahub
spec:
  ports:
    - port: 443
      targetPort: webhook-server
  selector:
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: kserve-controller-manager
    app.kubernetes.io/part-of: kserve
    control-plane: kserve-controller-manager
    controller-tools.k8s.io: "1.0"
  name: kserve-controller-manager
  namespace: opendatahub
spec:
  selector:
    matchLabels:
      app.kubernetes.io/part-of: kserve
      control-plane: kserve-controller-manager
      controller-tools.k8s.io: "1.0"
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        app.kubernetes.io/name: kserve-controller-manager
        app.kubernetes.io/part-of: kserve
        control-plane: kserve-controller-manager
        controller-tools.k8s.io: "1.0"
    spec:
      containers:
        - args:
            - --leader-elect
          command:
            - /manager
          env:
            - name: SERVICE_CA_SIGNING_SECRET_NAME
              value: opendatahub-ca
            - name: SERVICE_CA_SIGNING_SECRET_NAMESPACE
              value: cert-manager
            - name: ISTIO_CA_CERTIFICATE_PATH
              value: /var/run/secrets/opendatahub/ca.crt
            - name: ENABLE_ISVC_CONTROLLER
              value: "false"
            - name: ENABLE_LLMISVC_CONTROLLER
              value: "true"
            - name: ENABLE_TRAINED_MODEL_CONTROLLER
              value: "false"
            - name: ENABLE_INFERENCE_GRAPH_CONTROLLER
              value: "false"
            - name: LLMISVC_AUTH_DISABLED
              value: "true"
            - name: LLMISVC_MONITORING_DISABLED
              value: "true"
            - name: LLMISVC_SCC_DISABLED
              value: "true"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: SECRET_NAME
              value: kserve-webhook-server-cert
          image: registry.redhat.io/rhoai/odh-kserve-controller-rhel9@sha256:ef4d6c03cd06d18e52a0634d9bb9c2d2ed8223a11d9d583c4334cc92573849b5
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 30
            timeoutSeconds: 5
          name: manager
          ports:
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          volumeMounts:
            - mountPath: /tmp/k8s-webhook-server/serving-certs
              name: cert
              readOnly: true
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: kserve-controller-manager
      terminationGracePeriodSeconds: 10
      volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: kserve-webhook-server-cert
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kserve-webhook-server
  namespace: opendatahub
spec:
  commonName: kserve-webhook-server-service.opendatahub.svc
  dnsNames:
    - kserve-webhook-server-service.opendatahub.svc
    - kserve-webhook-server-service.opendatahub.svc.cluster.local
  duration: 8760h
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: opendatahub-ca-issuer
  renewBefore: 720h
  secretName: kserve-webhook-server-cert
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kserve-controller-manager
  namespace: opendatahub
spec:
  ingress:
    - {}
  podSelector:
    matchLabels:
      control-plane: kserve-controller-manager
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-decode-template
  namespace: opendatahub
spec:
  template:
    containers:
      - args:
          - "if [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\neval \"vllm serve /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8001 \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --enable-ssl-refresh \\\n  --ssl-certfile /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 10
        name: main
        ports:
          - containerPort: 8001
            protocol: TCP
        readinessProbe:
          failureThreshold: 60
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    initContainers:
      - args:
          - --port=8000
          - --vllm-port=8001
          - --connector=nixlv2
          - --secure-proxy=true
          - --cert-path=/var/run/kserve/tls
          - --decoder-use-tls=true
          - --prefiller-use-tls=true
          - --enable-ssrf-protection=true
          - --pool-group=inference.networking.x-k8s.io
        env:
          - name: INFERENCE_POOL_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: SSL_CERT_DIR
            value: /var/run/kserve/tls:/var/run/secrets/kubernetes.io/serviceaccount:/etc/pki/tls/certs
        image: registry.redhat.io/rhoai/odh-llm-d-routing-sidecar-rhel9@sha256:7f93742da18df2ce220cd8d6a0310c18af6fe04905c83f23d022e065716ebd88
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 10
        name: llm-d-routing-sidecar
        ports:
          - containerPort: 8000
            protocol: TCP
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        resources: {}
        restartPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 1Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-decode-worker-data-parallel
  namespace: opendatahub
spec:
  template:
    containers:
      - args:
          - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=0\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8001 \\\n  --api-server-count ${VLLM_API_SERVER_COUNT:-8} \\\n  {{- if .Spec.Parallelism.Expert -}}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Parallelism.Tensor -}}--tensor-parallel-size {{ .Spec.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Parallelism.DataRPCPort }}{{ .Spec.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 300
          periodSeconds: 10
          timeoutSeconds: 10
        name: main
        ports:
          - containerPort: 8001
            protocol: TCP
        readinessProbe:
          failureThreshold: 60
          httpGet:
            path: /health
            port: 8001
            scheme: HTTPS
          initialDelaySeconds: 200
          periodSeconds: 30
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
              - NET_RAW
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    initContainers:
      - args:
          - --port=8000
          - --vllm-port=8001
          - --connector=nixlv2
          - --secure-proxy=true
          - --cert-path=/var/run/kserve/tls
          - --decoder-use-tls=true
          - --prefiller-use-tls=true
          - --enable-ssrf-protection=true
          - --pool-group=inference.networking.x-k8s.io
        env:
          - name: INFERENCE_POOL_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: SSL_CERT_DIR
            value: /var/run/kserve/tls:/var/run/secrets/kubernetes.io/serviceaccount:/etc/pki/tls/certs
        image: registry.redhat.io/rhoai/odh-llm-d-routing-sidecar-rhel9@sha256:7f93742da18df2ce220cd8d6a0310c18af6fe04905c83f23d022e065716ebd88
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 10
        name: llm-d-routing-sidecar
        ports:
          - containerPort: 8000
            protocol: TCP
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        restartPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 8Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
  worker:
    containers:
      - args:
          - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=$(( ${LWS_WORKER_INDEX:-0} * {{ or .Spec.Parallelism.DataLocal 1 }} ))\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8001 \\\n  {{- if .Spec.Parallelism.Expert }}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Parallelism.Tensor }}--tensor-parallel-size {{ .Spec.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Parallelism.DataRPCPort }}{{ .Spec.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --headless \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        name: main
        ports:
          - containerPort: 8001
            protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
              - NET_RAW
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 8Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-prefill-template
  namespace: opendatahub
spec:
  prefill:
    template:
      containers:
        - args:
            - "if [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\neval \"vllm serve /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port \"8000\" \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --enable-ssl-refresh \\\n  --ssl-certfile /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile /var/run/kserve/tls/tls.key\""
          command:
            - /bin/bash
            - -c
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 10
          name: main
          ports:
            - containerPort: 8000
              protocol: TCP
          readinessProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache
            - mountPath: /var/run/kserve/tls
              name: tls-certs
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - emptyDir: {}
          name: home
        - emptyDir:
            medium: Memory
            sizeLimit: 1Gi
          name: dshm
        - emptyDir: {}
          name: model-cache
        - name: tls-certs
          secret:
            secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-prefill-worker-data-parallel
  namespace: opendatahub
spec:
  prefill:
    template:
      containers:
        - args:
            - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=0\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8000 \\\n  --api-server-count ${VLLM_API_SERVER_COUNT:-8} \\\n  {{- if .Spec.Prefill.Parallelism.Expert -}}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Prefill.Parallelism.Tensor -}}--tensor-parallel-size {{ .Spec.Prefill.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Prefill.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Prefill.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Prefill.Parallelism.DataRPCPort }}{{ .Spec.Prefill.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
          command:
            - /bin/bash
            - -c
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 300
            periodSeconds: 10
            timeoutSeconds: 10
          name: main
          ports:
            - containerPort: 8000
              protocol: TCP
          readinessProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 200
            periodSeconds: 30
            timeoutSeconds: 5
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - IPC_LOCK
                - SYS_RAWIO
                - NET_RAW
              drop:
                - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache
            - mountPath: /var/run/kserve/tls
              name: tls-certs
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - emptyDir: {}
          name: home
        - emptyDir:
            medium: Memory
            sizeLimit: 8Gi
          name: dshm
        - emptyDir: {}
          name: model-cache
        - name: tls-certs
          secret:
            secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
    worker:
      containers:
        - args:
            - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=$(( ${LWS_WORKER_INDEX:-0} * {{ or .Spec.Prefill.Parallelism.DataLocal 1 }} ))\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8000 \\\n  {{- if .Spec.Prefill.Parallelism.Expert }}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Prefill.Parallelism.Tensor }}--tensor-parallel-size {{ .Spec.Prefill.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Prefill.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Prefill.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Prefill.Parallelism.DataRPCPort }}{{ .Spec.Prefill.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --headless \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
          command:
            - /bin/bash
            - -c
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
          imagePullPolicy: IfNotPresent
          name: main
          ports:
            - containerPort: 8000
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - IPC_LOCK
                - SYS_RAWIO
                - NET_RAW
              drop:
                - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache
            - mountPath: /var/run/kserve/tls
              name: tls-certs
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - emptyDir: {}
          name: home
        - emptyDir:
            medium: Memory
            sizeLimit: 8Gi
          name: dshm
        - emptyDir: {}
          name: model-cache
        - name: tls-certs
          secret:
            secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-router-route
  namespace: opendatahub
spec:
  router:
    route:
      http:
        spec:
          parentRefs:
            - group: gateway.networking.k8s.io
              kind: Gateway
              name: '{{ .GlobalConfig.IngressGatewayName }}'
              namespace: '{{ .GlobalConfig.IngressGatewayNamespace }}'
          rules:
            - backendRefs:
                - group: inference.networking.x-k8s.io
                  kind: InferencePool
                  name: '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
                  port: 8000
                  weight: 1
              filters:
                - type: URLRewrite
                  urlRewrite:
                    path:
                      replacePrefixMatch: /v1/completions
                      type: ReplacePrefixMatch
              matches:
                - path:
                    type: PathPrefix
                    value: /{{ .ObjectMeta.Namespace }}/{{ .ObjectMeta.Name }}/v1/completions
              timeouts:
                backendRequest: 0s
                request: 0s
            - backendRefs:
                - group: inference.networking.x-k8s.io
                  kind: InferencePool
                  name: '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
                  port: 8000
                  weight: 1
              filters:
                - type: URLRewrite
                  urlRewrite:
                    path:
                      replacePrefixMatch: /v1/chat/completions
                      type: ReplacePrefixMatch
              matches:
                - path:
                    type: PathPrefix
                    value: /{{ .ObjectMeta.Namespace }}/{{ .ObjectMeta.Name }}/v1/chat/completions
              timeouts:
                backendRequest: 0s
                request: 0s
            - backendRefs:
                - kind: Service
                  name: '{{ ChildName .ObjectMeta.Name `-kserve-workload-svc` }}'
                  port: 8000
                  weight: 1
              filters:
                - type: URLRewrite
                  urlRewrite:
                    path:
                      replacePrefixMatch: /
                      type: ReplacePrefixMatch
              matches:
                - path:
                    type: PathPrefix
                    value: /{{ .ObjectMeta.Namespace }}/{{ .ObjectMeta.Name }}
              timeouts:
                backendRequest: 0s
                request: 0s
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-scheduler
  namespace: opendatahub
spec:
  router:
    scheduler:
      pool:
        spec:
          extensionRef:
            failureMode: FailOpen
            kind: Service
            name: '{{ ChildName .ObjectMeta.Name `-epp-service` }}'
          selector: {}
          targetPortNumber: 8000
      template:
        containers:
          - args:
              - --pool-name
              - '{{ ChildName .ObjectMeta.Name `-inference-pool` }}'
              - --pool-namespace
              - '{{ .ObjectMeta.Namespace }}'
              - --pool-group
              - inference.networking.x-k8s.io
              - --zap-encoder
              - json
              - --grpc-port
              - "9002"
              - --grpc-health-port
              - "9003"
              - --secure-serving
              - --model-server-metrics-scheme
              - https
              - --cert-path
              - /var/run/kserve/tls
            env:
              - name: SSL_CERT_DIR
                value: /var/run/kserve/tls:/var/run/secrets/kubernetes.io/serviceaccount:/etc/pki/tls/certs
            image: registry.redhat.io/rhoai/odh-llm-d-inference-scheduler-rhel9@sha256:905b704f7d9c68af1827bba7c6eefec22d269a9084791952221eb1a1d10e32c7
            imagePullPolicy: IfNotPresent
            livenessProbe:
              failureThreshold: 3
              grpc:
                port: 9003
                service: envoy.service.ext_proc.v3.ExternalProcessor
              initialDelaySeconds: 5
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            name: main
            ports:
              - containerPort: 9002
                name: grpc
                protocol: TCP
              - containerPort: 9003
                name: grpc-health
                protocol: TCP
              - containerPort: 9090
                name: metrics
                protocol: TCP
              - containerPort: 5557
                name: zmq
                protocol: TCP
            readinessProbe:
              failureThreshold: 3
              grpc:
                port: 9003
                service: envoy.service.ext_proc.v3.ExternalProcessor
              initialDelaySeconds: 30
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            resources:
              requests:
                cpu: 256m
                memory: 500Mi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              seccompProfile:
                type: RuntimeDefault
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: FallbackToLogsOnError
            volumeMounts:
              - mountPath: /var/run/kserve/tls
                name: tls-certs
                readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        terminationGracePeriodSeconds: 30
        volumes:
          - name: tls-certs
            secret:
              secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-template
  namespace: opendatahub
spec:
  template:
    containers:
      - args:
          - "if [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\neval \"vllm serve /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8000 \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --enable-ssl-refresh \\\n  --ssl-certfile /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 10
        name: main
        ports:
          - containerPort: 8000
            protocol: TCP
        readinessProbe:
          failureThreshold: 60
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 1Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    description: vLLM AMD GPU LLMInferenceServiceConfig for LLMInferenceService.
    openshift.io/display-name: vLLM AMD GPU LLMInferenceServiceConfig
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-template-amd-rocm
  namespace: opendatahub
spec:
  template:
    containers:
      - image: registry.redhat.io/rhaiis/vllm-rocm-rhel9@sha256:e345cf9453afae0d3c2afe2f7fb9be8fac772f46593b873e925d38ae2b3ee537
        name: main
---
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  annotations:
    serving.kserve.io/well-known-config: "true"
  name: kserve-config-llm-worker-data-parallel
  namespace: opendatahub
spec:
  template:
    containers:
      - args:
          - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n\n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n\n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n\n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n\n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=0\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8000 \\\n  --api-server-count ${VLLM_API_SERVER_COUNT:-8} \\\n  {{- if .Spec.Parallelism.Expert -}}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Parallelism.Tensor -}}--tensor-parallel-size {{ .Spec.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Parallelism.DataRPCPort }}{{ .Spec.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 300
          periodSeconds: 10
          timeoutSeconds: 10
        name: main
        ports:
          - containerPort: 8000
            protocol: TCP
        readinessProbe:
          failureThreshold: 60
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 200
          periodSeconds: 30
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
              - NET_RAW
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 8Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
  worker:
    containers:
      - args:
          - "# In some versions, ZMQ bind doesn't resolve the address through DNS\n# Retry DP_ADDRESS resolution (configurable attempts, default 30)\nRESOLVE_ATTEMPTS=${DP_ADDRESS_RESOLVE_ATTEMPTS:-30}\nfor ((i=1; i<=RESOLVE_ATTEMPTS; i++)); do\n  DP_ADDRESS=$(getent hosts ${LWS_LEADER_ADDRESS} | cut -d' ' -f1)\n  if [ -n \"$DP_ADDRESS\" ]; then\n    echo \"DP_ADDRESS=${DP_ADDRESS} (resolved on attempt $i)\"\n    break\n  else\n    echo \"DP_ADDRESS resolution failed on attempt $i, retrying...\"\n    sleep 1\n  fi\ndone\n\nif [ -z \"$DP_ADDRESS\" ]; then\n  echo \"WARNING: Failed to resolve DP_ADDRESS after ${RESOLVE_ATTEMPTS} attempts, falling back to LWS_LEADER_ADDRESS\"\n  DP_ADDRESS=${LWS_LEADER_ADDRESS}\n  echo \"DP_ADDRESS=${DP_ADDRESS} (fallback)\"\nfi\n\nif [ \"$KSERVE_INFER_ROCE\" = \"true\" ]; then\n  echo \"Trying to infer RoCE configs ... \"\n  grep -H . /sys/class/infiniband/*/ports/*/gids/* 2>/dev/null\n  grep -H . /sys/class/infiniband/*/ports/*/gid_attrs/types/* 2>/dev/null\n\n  cat /proc/driver/nvidia/params\n\n  KSERVE_INFER_IB_GID_INDEX_GREP=${KSERVE_INFER_IB_GID_INDEX_GREP:-\"RoCE v2\"}\n\n  echo \"[Infer RoCE] Discovering active HCAs ...\"\n  active_hcas=()\n  # Loop through all mlx5 devices found in sysfs\n  for hca_dir in /sys/class/infiniband/mlx5_*; do\n      # Ensure it's a directory before proceeding\n      if [ -d \"$hca_dir\" ]; then\n          hca_name=$(basename \"$hca_dir\")\n          port_state_file=\"$hca_dir/ports/1/state\" # Assume port 1\n          type_file=\"$hca_dir/ports/1/gid_attrs/types/*\"\n\n          echo \"[Infer RoCE] Check if the port state file ${port_state_file} exists and contains 'ACTIVE'\"\n          if [ -f \"$port_state_file\" ] && grep -q \"ACTIVE\" \"$port_state_file\" && grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" ${type_file} 2>/dev/null; then\n              echo \"[Infer RoCE] Found active HCA: $hca_name\"\n              active_hcas+=(\"$hca_name\")\n          else\n              echo \"[Infer RoCE] Skipping inactive or down HCA: $hca_name\"\n          fi\n      fi\n  done\n\n  ucx_hcas=()\n  for hca in \"${active_hcas[@]}\"; do\n    ucx_hcas+=(\"${hca}:1\")\n  done\n\n  # Check if we found any active HCAs\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      # Join the array elements with a comma\n      hcas=$(IFS=,; echo \"${active_hcas[*]}\")\n      echo \"[Infer RoCE] Setting active HCAs: ${hcas}\"\n      export NCCL_IB_HCA=${NCCL_IB_HCA:-${hcas}}\n      export NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST:-${ucx_hcas}}\n      export UCX_NET_DEVICES=${UCX_NET_DEVICES:-${ucx_hcas}}\n\n      echo \"[Infer RoCE] NCCL_IB_HCA=${NCCL_IB_HCA}\"\n      echo \"[Infer RoCE] NVSHMEM_HCA_LIST=${NVSHMEM_HCA_LIST}\"\n  else\n      echo \"[Infer RoCE] WARNING: No active RoCE HCAs found. NCCL_IB_HCA will not be set.\"\n  fi\n\n  if [ ${#active_hcas[@]} -gt 0 ]; then\n      echo \"[Infer RoCE] Finding GID_INDEX for each active HCA (SR-IOV compatible)...\"\n\n      # For SR-IOV environments, find the most common IPv4 RoCE v2 GID index across all HCAs\n      declare -A gid_index_count\n      declare -A hca_gid_index\n      \n      for hca_name in \"${active_hcas[@]}\"; do\n          echo \"[Infer RoCE] Processing HCA: ${hca_name}\"\n          \n          # Find all RoCE v2 IPv4 GIDs for this HCA and count by index\n          for tpath in /sys/class/infiniband/${hca_name}/ports/1/gid_attrs/types/*; do\n              if grep -q \"${KSERVE_INFER_IB_GID_INDEX_GREP}\" \"$tpath\" 2>/dev/null; then\n                  idx=$(basename \"$tpath\")\n                  gid_file=\"/sys/class/infiniband/${hca_name}/ports/1/gids/${idx}\"\n                  # Check for IPv4 GID (contains ffff:)\n                  if [ -f \"$gid_file\" ] && grep -q \"ffff:\" \"$gid_file\"; then\n                      gid_value=$(cat \"$gid_file\" 2>/dev/null || echo \"\")\n                      echo \"[Infer RoCE] Found IPv4 RoCE v2 GID for ${hca_name}: index=${idx}, gid=${gid_value}\"\n                      hca_gid_index[\"${hca_name}\"]=\"${idx}\"\n                      gid_index_count[\"${idx}\"]=$((${gid_index_count[\"${idx}\"]} + 1))\n                      break  # Use first found IPv4 GID per HCA\n                  fi\n              fi\n          done\n      done\n\n      # Find the most common GID index (most likely to be consistent across nodes)\n      best_gid_index=\"\"\n      max_count=0\n      for idx in \"${!gid_index_count[@]}\"; do\n          count=${gid_index_count[\"${idx}\"]}\n          echo \"[Infer RoCE] GID_INDEX ${idx} found on ${count} HCAs\"\n          if [ $count -gt $max_count ]; then\n              max_count=$count\n              best_gid_index=\"$idx\"\n          fi\n      done\n\n      # Use deterministic fallback if counts are equal - prefer lower index number  \n      if [ ${#gid_index_count[@]} -gt 1 ]; then\n          echo \"[Infer RoCE] Multiple GID indices found, selecting most common: ${best_gid_index}\"\n          # If there's a tie, prefer index 3 as it's most common in SR-IOV setups\n          if [ -n \"${gid_index_count['3']}\" ] && [ \"${gid_index_count['3']}\" -eq \"$max_count\" ]; then\n              best_gid_index=\"3\"\n              echo \"[Infer RoCE] Using deterministic fallback: GID_INDEX=3 (SR-IOV standard)\"\n          fi\n      fi\n\n      # Check if GID_INDEX is already set via environment variables\n      if [ -n \"${NCCL_IB_GID_INDEX}\" ]; then\n          echo \"[Infer RoCE] Using pre-configured NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX} from environment\"\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$NCCL_IB_GID_INDEX}\n          echo \"[Infer RoCE] Using hardcoded GID_INDEX=${NCCL_IB_GID_INDEX} for NCCL, NVSHMEM, and UCX\"\n      elif [ -n \"$best_gid_index\" ]; then\n          echo \"[Infer RoCE] Selected GID_INDEX: ${best_gid_index} (found on ${max_count} HCAs)\"\n          \n          export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-$best_gid_index}\n          export NVSHMEM_IB_GID_INDEX=${NVSHMEM_IB_GID_INDEX:-$best_gid_index}\n          export UCX_IB_GID_INDEX=${UCX_IB_GID_INDEX:-$best_gid_index}\n          \n          echo \"[Infer RoCE] Exported GID_INDEX=${best_gid_index} for NCCL, NVSHMEM, and UCX\"\n      else\n          echo \"[Infer RoCE] ERROR: No valid IPv4 ${KSERVE_INFER_IB_GID_INDEX_GREP} GID_INDEX found on any HCA.\"\n      fi\n  else\n      echo \"[Infer RoCE] No active HCAs found, skipping GID_INDEX inference.\"\n  fi\nfi\n\nSTART_RANK=$(( ${LWS_WORKER_INDEX:-0} * {{ or .Spec.Parallelism.DataLocal 1 }} ))\neval \"vllm serve \\\n  /mnt/models \\\n  --served-model-name \"{{ .Spec.Model.Name }}\" \\\n  --port 8000 \\\n  {{- if .Spec.Parallelism.Expert }}--enable-expert-parallel{{- end }} \\\n  {{- if .Spec.Parallelism.Tensor }}--tensor-parallel-size {{ .Spec.Parallelism.Tensor }}{{- end }} \\\n  --data-parallel-size {{ or .Spec.Parallelism.Data 1 }} \\\n  --data-parallel-size-local {{ or .Spec.Parallelism.DataLocal 1 }} \\\n  --data-parallel-address ${DP_ADDRESS} \\\n  --data-parallel-rpc-port {{ if .Spec.Parallelism.DataRPCPort }}{{ .Spec.Parallelism.DataRPCPort }}{{ else }}5555{{- end }} \\\n  --data-parallel-start-rank $START_RANK \\\n  ${VLLM_ADDITIONAL_ARGS} \\\n  --trust-remote-code \\\n  --headless \\\n  --enable-ssl-refresh \\\n  --ssl-certfile \\\n  /var/run/kserve/tls/tls.crt \\\n  --ssl-keyfile \\\n  /var/run/kserve/tls/tls.key\""
        command:
          - /bin/bash
          - -c
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ec799bb5eeb7e25b4b25a8917ab5161da6b6f1ab830cbba61bba371cffb0c34d
        imagePullPolicy: IfNotPresent
        name: main
        ports:
          - containerPort: 8000
            protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RAWIO
              - NET_RAW
            drop:
              - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache
          - mountPath: /var/run/kserve/tls
            name: tls-certs
            readOnly: true
    terminationGracePeriodSeconds: 30
    volumes:
      - emptyDir: {}
        name: home
      - emptyDir:
          medium: Memory
          sizeLimit: 8Gi
        name: dshm
      - emptyDir: {}
        name: model-cache
      - name: tls-certs
        secret:
          secretName: '{{ ChildName .ObjectMeta.Name `-kserve-self-signed-certs` }}'
